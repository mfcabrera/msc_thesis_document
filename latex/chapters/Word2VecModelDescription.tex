\chapter{Description of the Word2Vec Model }
\label{chap:word2vec_description}


\section{Introduction}
As described in  previous chapter,  continuous space language  models have
demonstrated good results across a variety of tasks in the field of NLP.  As pointed out by
the authors, these continuous word vector representation achieves a level of
generalization not possible with class traditional $n$-gram based models
\cite{conf/icassp/MikolovKBGC09}. In addition to the language model,  word vector
representations are learned in the initial layers of a \ac{NN}. These
induced representations  have been used successfully to improve the
performance of existing \ac{NLP} tasks \cite{collobert:2008}
\cite{Turian:2010:WRS:1858681.1858721}. 

As discussed in section \ref{sec:mikolov-neural-net-model},  One of the way to learn a
\ac{NNLM} is to learn the word vector using  a \ac{NN} with a single hidden
layer. Then, these word vectors are used to  train another the final
\ac{NNLM} \cite{conf/icassp/MikolovKBGC09}.   
What is particular about this architecture is that they learn the  word
vectors without  even constructing the complete language model.  This means
in practice that the two step are performed separately. 

This chapter describes in detail a recent approach to efficiently learn
high-dimensional word representation \cite{DBLP:journals/corr/abs-1301-3781}.
In other words, focusing only on the first part of the previously described 
architecture. It has been found that the word vectors learned in this manner,
not only capture linguistic regularities of words  but also semantic
relationships. As the word vector generated from this architecture is used in
through this work, therefore it is necessary to describe this model with more
detail.

% making it  necessary to describe
%%it properly.  
The rest of the chapter is structured as follows blah!

\section{Modern Neural Network Language Model Architectures}

In order to understand  \textit{Word2vec}'s it is necessary to study the
models on which it is built upon. Word2vec as both an improvement and
as well as a simplification of a full-fledged \ac{NNLM}.
% Therefore, this section introduces
%briefly the most common  architectures and their more important
%characteristics.

\subsection{Model Complexity}

One of the main reason behind the development of \textit{Word2vec} was the
need of obtaining good word vector representation with low training complexity cost.







A neural probabilistic language model

% Note that Bengio [1] claims that training word features
% together with neural network language model is better than
% independent training of features and n-gram NN LM. In our
% approach, we consider the first network as a feature extractor
% and do not train it together with the n-gram net. We suppose
% that this can prevent word features from learning cache-like
% relationships in the data, which tend to heavily optimize per-
% plexity, but not recognition accuracy (Goodman [8]).


\section{Word2Vec}

% What is word 2 vec ... !


% Summary of the model
%% Architecture
%% Algorithm
%% Other important parameter
%%% Subsampling
%%% Window


\subsection{\ac{CBOW} Model}
\subsection{Skip-Gram Model}


\section{Optimization Strategies used by Word2vec}
\label{sec:strategies_improve_time}

\subsection{Huffman Coding}
\label{sec:huffman_coding}

Huffman coding is an entropy  encoding algorithm used for lossless data
compression.  It was developed by David A. Huffman while he was a Ph.D.
student at MIT. \cite{huf52}.

Huffan cioding is an algorithm to proce an  optimal symbol code $C$ for a given 
\emph{pmf} $ p = (p_1,....,p_n) $.  Huffman coding builds a tree orderded by frequency.



%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../main.tex"
%%% End:
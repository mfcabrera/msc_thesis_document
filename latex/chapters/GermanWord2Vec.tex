\chapter{Empirical Study of Word Vectors for the German Language}
\label{chapter:wor2vec_german}

%% In this chapter 

%% - What we do in this chapter 

%% - Rationale why we do that how we do it and why

%% - Experiments

%% Datasizes and so on

%% - Embedding using t-SNE of German wikipedia (top X)

%% - Embedding using t-SNE of GiniData using top X

%% - Table of the experiments

%% - Analysis of the results 

The previous chapters introduced the concepts and related work  in the fields
of  \ac{NLP}, \ac{NNLM} and vector representation of words. This chapter will
focus on the application of a particular model, namely \textit{Word2Vec}, to
the German language.  

There art two main reasons that justify the study of the behavior of word
vector embeddings in a language as morphologically rich and moderate inflective
as German.  First  as reported in previous work, n-gram backoff language models, the state of the art
technique, do not perform  as well for inflectional  languages as they work
for English \cite{conf/icassp/MikolovKBGC09}. Recent alternatives
n-gram based models are \ac{NNLM} \cite{Bengio:2003:NPL:944919.944966} described
in detail in CHAPTER [TODO:REF]. \ac{NNLM} had been shown to perform well \cite{conf/icassp/MikolovKBGC09}



 [Thurian and
Other people papers listed on Mikolov]

- This work have done in english (not studied in depth in high inflectional
languages) however there are not study which theoritically explains what 

- There are some evidence of good performance morphologucally rich (highly
inflective) lanaguage 

% - [On the other hand, n-gram backoff models do not work
% as well for inﬂectional languages as they work for
% English] \cite{conf/icassp/MikolovKBGC09} 

- Several alternative model has been proposed based on \ac{NNLM}

- As the word embeddings are based also en \ac{NNLM} it is expected that
thereofore works better. 

- We use in our work German is a morpholgically rich and moderate inflected
language (CITATION). We want to test evaluate the performance of this
particular word embeddings. 

Similar to the English study we define a set of tasks 

To evaluate a language model is necessary define a set of tasks 
For that purpose, a equivalent subset of tasks targeted at the German language were
defined in the same maner as by Mikolov et al.
\cite{DBLP:journals/corr/abs-1301-3781} do for the English.  


It is challenging to test and compare different language models base just by
comparing their algorithm description, furthermore if for the language of
choosing no standard data set exist. For that purpose a equivalent  to the
one defined by  has been created. 

This chapter will describe the experiments performed as well as the data sets
used to test the model. Additionally, a comparison between

To test this model for  German,  a equivalent set of semantic and
syntantic tasks were created in a similar way to the ones defined by Mikolov
et al \cite{MikolovSCCD13}.

This chapter describes the experiments and and tasks used to measure  

- [Neural networks seem to be very useful for tasks where
we struggle with inflectional languages and limited data
amounts. While performance of our implementation is not
high,  presents various optimization techniques that lead to
significant improvements in required computational time for
training and lattice re-scoring.] - \cite{conf/icassp/MikolovKBGC09}




\section{Fuck Me}



There is no need for a latex introduction since there is plenty of literature out there.


It is very difficult, if not impossible, to compare different machine learning techniques just by following their theoretical description. The same holds for the numerous language modeling techniques: almost every one of them is well-motivated, and some of them even have theoretical explanation why a given technique is optimal, under certain assumptions. The problem is that many of such assumptions are not satisfied in practice, when real data are used.
Comparison of advanced language modeling techniques is usually limited by some of these factors:
%% • private data sets that do not allow experiments to be repeated are used
%% • ad hoc preprocessing is used that favours the proposed technique, or completely
%% artificial data sets are used
%% • comparison to proper baseline is completely missing
%% • baseline technique is not tuned for the best performance
%% • in comparison, it is falsely claimed that technique X is the state of the art (where X is usually n-gram model)
%%  possible comparison to other advanced techniques is done poorly, by citing results achieved on different data sets, or simply by falsely claiming that the other techniques are too complex 

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../main.tex"
%%% End:
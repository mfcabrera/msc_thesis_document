\chapter{Empirical Study of Word Vectors for the German Language}
\label{chapter:wor2vec_german}

%% In this chapter 

%% - What we do in this chapter 

%% - Rationale why we do that how we do it and why

%% - Experiments

%% Datasizes and so on

%% - Embedding using t-SNE of German wikipedia (top X)

%% - Embedding using t-SNE of GiniData using top X

%% - Table of the experiments

%% - Analysis of the results 

The previous chapters introduced the concepts and previous work in the field
of  \ac{NLP}, \ac{NNLM} and vector representation of words. This chapter will
focus on the application of a particular model, namely \textit{Word2Vec}, to
the German language.  

Similar to the English study we define a set of tasks 


To evaluate a language model is necessary define a set of tasks 
For that purpose, a equivalent subset of tasks targeted at the German language were
defined in the same maner as by Mikolov et al.
\cite{DBLP:journals/corr/abs-1301-3781} do for the English.  


It is challenging to test and compare different language models base just by
comparing their algorithm description, furthermore if for the language of
choosing no standard data set exist. For that purpose a equivalent  to the
one defined by  has been created. 

This chapter will describe the experiments performed as well as the data sets
used to test the model. Additionally, a comparison between

To test this model for  German,  a equivalent set of semantic and
syntantic tasks were created in a similar way to the ones defined by Mikolov
et al \cite{MikolovSCCD13}.

This chapter describes the experiments and and tasks used to measure  



\section{Fuck Me}



There is no need for a latex introduction since there is plenty of literature out there.


It is very difficult, if not impossible, to compare different machine learning techniques just by following their theoretical description. The same holds for the numerous language modeling techniques: almost every one of them is well-motivated, and some of them even have theoretical explanation why a given technique is optimal, under certain assumptions. The problem is that many of such assumptions are not satisfied in practice, when real data are used.
Comparison of advanced language modeling techniques is usually limited by some of these factors:
%% • private data sets that do not allow experiments to be repeated are used
%% • ad hoc preprocessing is used that favours the proposed technique, or completely
%% artificial data sets are used
%% • comparison to proper baseline is completely missing
%% • baseline technique is not tuned for the best performance
%% • in comparison, it is falsely claimed that technique X is the state of the art (where X is usually n-gram model)
%%  possible comparison to other advanced techniques is done poorly, by citing results achieved on different data sets, or simply by falsely claiming that the other techniques are too complex 

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../main.tex"
%%% End:
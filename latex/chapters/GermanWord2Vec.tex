\chapter{Empirical Study of Word Vectors for the German Language}
\label{chapter:wor2vec_german}

%% In this chapter 

%% - What we do in this chapter 

%% - Rationale why we do that how we do it and why

%% - Short descrioption of Experiments

%% Tasks for English and German and comparisson of Gramatical and semantical differences

%% Datasizes and so on

%% - Embedding using t-SNE of German wikipedia (top X)

%% - Embedding using t-SNE of GiniData using top X

%% - Table of the experiments

%% - Analysis of the results 

The previous chapters introduced the concepts and related work  in the fields
of  \ac{NLP}, \ac{NNLM} and vector representation of words. This chapter will
focus on the application of a particular model, namely \textit{Word2vec}, to
the German language.  

% There art two main reasons that justify the study of the behavior of word
% vector representation in a language as morphologically rich and  inflective
% as German.

Although that  evidence that word vector representation might work
well in other languages other than English has been presented
\cite{DBLP:journals/corr/MikolovLS13}, to the date  there are not in depth studies or empirical evaluations of the
performance of word representation for such languages,  in similar or equivalent tasks as the
ones performed by \cite{DBLP:journals/corr/abs-1301-3781} for the English
language. 

Previous work has shown that  n-gram back-off language models, the state of the art
technique, does not perform  as well for inflectional  languages as they work
for English \cite{conf/icassp/MikolovKBGC09}.  It has also been shown that
\ac{NNLM} captures  better the complexity of such languages
\cite{conf/icassp/MikolovKBGC09}\cite{DBLP:journals/corr/MikolovLS13}.
Therefore, it is possible that word vectors generated by such model  will 
improve  and simplify many \ac{NLP} applications in
these languages as they have been shown to do in
English \cite{collobert:2008} \cite{Turian:2010:WRS:1858681.1858721}.
Consequently, making the evaluation of the quality of the word vectors in
these languages very attractive.

The rest of the chapter is divided as follows: The first part describes the
selected tasks, the rationale behind the adaption made to fit the German
language and the dataset. The second part describes the training approach as well as the most
important results. It includes  a performance comparison of the word
vectors in German with their counterpart in English as well as  the evaluation of  some corpus
preprocessing approaches that might affect the performance of the word
representation for German.

%  In this chapter we will
% describe a set of tasks adapted from \cite{DBLP:journals/corr/abs-1301-3781}
% to better match the characteristics of German language. In addition we will
% compare  the performance with English language.


%  As word vectors are generated
% by \ac{NNLM} \cite{Bengio:2003:NPL:944919.944966}, it is expected therefore,
% t

% First  as reported in previous work, n-gram backoff language models, the state of the art
% technique, do not perform  as well for inflectional  languages as they work
% for English \cite{conf/icassp/MikolovKBGC09}. Recent alternatives
% n-gram based models are \ac{NNLM} \cite{Bengio:2003:NPL:944919.944966} described
% in detail in CHAPTER [TODO:REF]. Besides the language model,  \ac{NNLM}
% generate the word  vector representations .

% in this type of languages \cite{conf/icassp/MikolovKBGC09}\cite{DBLP:journals/corr/MikolovLS13}.
% Second, 

% As mentioned before,  vector representations have shown the ability of
% improve existing \ac{NLP} taks 

% -  There is no studying comparing performances in two different languages and
% how the differences between the language and the standard language affects. 



%  [Thurian and
% Other people papers listed on Mikolov]

% - This work have done in english (not studied in depth in high inflectional
% languages) however there are not study which theoritically explains what 

% - There are some evidence of good performance morphologucally rich (highly
% inflective) lanaguage 

% %
%- [On the other hand, n-gram backoff models do not work
% as well for inﬂectional languages as they work for
% English] \cite{conf/icassp/MikolovKBGC09} 

%  Several alternative model has been proposed based on \ac{NNLM}

% - As the word embeddings are based also en \ac{NNLM} it is expected that
% thereofore works better. 

% - We use in our work German is a morpholgically rich and moderate inflected
% language (CITATION). We want to test evaluate the performance of this
% particular word embeddings. 

% Similar to the English study we define a set of tasks 

% To evaluate a language model is necessary define a set of tasks 
% For that purpose, a equivalent subset of tasks targeted at the German language were
% defined in the same maner as by Mikolov et al.
% \cite{DBLP:journals/corr/abs-1301-3781} do for the English.  


% It is challenging to test and compare different language models base just by
% comparing their algorithm description, furthermore if for the language of
% choosing no standard dataset exist. For that purpose a equivalent  to the
% one defined by  has been created. 

% This chapter will describe the experiments performed as well as the datasets
% used to test the model. Additionally, a comparison between

% To test this model for  German,  a equivalent set of semantic and
% syntantic tasks were created in a similar way to the ones defined by Mikolov
% et al \cite{MikolovSCCD13}.

% This chapter describes the experiments and and tasks used to measure  

% IMPORTANT ALSO FOR GINI SET - [Neural networks seem to be very useful for tasks where
% we struggle with inflectional languages and limited data
% amounts. While performance of our implementation is not
% high,  presents various optimization techniques that lead to
% significant improvements in required computational time for
% training and lattice re-scoring.] - \cite{conf/icassp/MikolovKBGC09}

\section{Task Description}
\label{sec:task_description}



In their original paper \cite{DBLP:journals/corr/abs-1301-3781}, the authors
introduced a comprehensive test that contained five types of semantic
questions. In the original dataset  there were 8869 semantic and 10675
syntactic questions. To create this set the authors compiled a list of
similar word pairs manually and then they generated the list by
connecting the word pairs. Table~\ref{tab:task_original_en} shows examples 
examples from each category of tasks. 

To evaluate the quality of the word vectors the question  are answer in the
following manner: given the word pairs ($a$, $b$) and ($c$, $d$) a question
is considered to be answered correctly if the for resulting vector $X$ the closest vector measured in
cosine similarity is  $vector(d)$. $X$ is calculated by applying the following
operation $X = vector(b) - vector(a) + vector(c)$.  Given two vectors $A$ and $B$, the
cosine similarity is calculated in the following way: 

$$\text{similarity} = \cos(\theta) = {A \cdot B \over \|A\| \|B\|}$$


 For example  for the words
($``small''$, $''smaller''$ ) and ($''big''$ ,$"bigger"$),
the $vector("bigger")$ should be the closest to the resulting
vector $X$. If the representation of the word in
vector space is good, then it is possible to find the answer using such
operation \cite{DBLP:journals/corr/abs-1301-3781}.

As discussed in the first part of this work, the word representations
capture different types of  similarities among words. Therefore, with the
operation described above,  what is  being
attempted is to answer the question ``\emph{What is the word that is similar to
c in the same sense as b is similar to a?}''. 

% To compare the quality of different versions of word vectors, previous papers typically use a table
% showing example words and their most similar words, and understand them intuitively. Although
% it is easy to show that word France is similar to Italy and perhaps some other countries, it is much
% more challenging when subjecting those vectors in a more complex similarity task, as follows. We
% follow previous observation that there can be many different types of similarities between words, for
% example, word big is similar to bigger in the same sense that small is similar to smaller. Example
% of another type of relationship can be word pairs big - biggest and small - smallest [20]. We further
% denote two pairs of words with the same relationship as a question, as we can ask: ”What is the
% word that is similar to small in the same sense as biggest is similar to big?”

In order to evaluate the performance of the word vectors in the same way but in other language a
similar set of questions  needs to be created. Two approaches might be used
to accomplish this. The words can be translated, either automatically or by hand, or the dataset can be
generated from scratch in a similar manner as the original was.  In this work
the translation was performed by hand using a German - English online dictionary \footnote{\url{http://dict.cc}} as guide and
verified by two native speakers.

Regardless of the method chosen, some tasks of this set are biased towards
the morphology of English and could not be easily mapped to another language,
and in particular to German. Therefore a simple translation would not be
enough to obtain a equivalent task set for another language. For that reason
each of the original 14 tasks was analyzed to either adapt it to German or
discard it completely. 


\renewcommand{\arraystretch}{1.3}

\begin{table}[h]

  \centering
  \caption{Examples of the original types of syntactic questions in the
    Semantic-Syntactic Word Relationship test as defined 
    by \cite{DBLP:journals/corr/abs-1301-3781}. }
  \label{tab:task_original_en}
  \small
  \begin{tabular}{ |l| |c|*{4}{c| |c| c | c }  }

  \hline           
  Type of Relationship &  \multicolumn{2}{c||}{Word Pair 1} &
  \multicolumn{2}{c|}{Word Pair 2} \\  \hline           
  Common capital city & Athens & Greece & Oslo  & Norway \\ 
  All capital cities  & Astana & Kazakhstan &  Harare & Zimbabwe  \\
  Currency & Angola & kwanza & Iran & rial \\  
  City-in-state  & Chicago & Illinois & Stockton & California \\  
  Man-Woman & brother & sister  & grandson & granddaughter \\  \hline  
  Adjective to adverb & apparent & apparently & rapid & rapidly  \\  
  Opposite & possibly & impossibly & ethical & unethical \\  
  Comparative & great & greater & tough & tougher \\  
  Superlative & easy & easiest & lucky & luckiest \\  
  Present participle & think & thinking & read & reading \\  
  Nationality adjective & Switzerland & Swiss & Cambodia  & Cambodian  \\  
  Past tense & walking & walked & swimming & swam \\ 
  Plural nouns  & mouse & mice & dollar & dollars \\  
  Past verbs & work & works & speak  & speaks  \\  \hline  
\end{tabular}
\end{table}

\section{Adapting the tasks to the German Language}
\label{sec:adapt_task_german_lang}

Many of  tasks from the original  semantic-syntactic set of task are language
dependent. In order to adapt them  to German,  each of type  of question was
evaluated.  Some of them could  be easily adapted to German
just by translating the respective word. However, some of the them either do
not make sense for German from the morphology point of view,  or they simply do not
apply for German. From the original 14 types of questions, the German version 
contains 10. A equivalent set was also constructed in parallel for English with
the purpose of performing a comparison. Below each of the tasks are described
in detail along with steps performed to adapt it to  German when possible.

\subsection{Common Capital and City and All Capital}
\label{sec:sub_sec_common_capital_country}

This group of question try to solve analogies between country and capitals.
The vector model is asked the semantic relationship \emph{Capital Country}.
For example:  ``\emph{What is the word that is similar to
Germany in the same sense as Paris is similar to France?}''. The model should
answer then the word produce a vector close to ``\emph{Berlin}''. The
difference between the two subsets is that common capital contains
 the 22 most ``\textit{common}'' country - capital word pairs. However the
 criteria for the selection is not defined by the authors of the set. The
 other subset contains country - capital pairs of less known countries such
 as \textit{Ghana  -  Accra} and  \textit{Harare -  Zimbabwe}.

For this task the adaptation was done by simply translating the names from
the English ones to their German counterparts. 

\subsection{Currency}
\label{sec:sub_sec_currency}
For the currency section all of the pairs were translated using the Wikipedia\footnote{\url{http://de.wikipedia.org}}
as source for the information. 

\subsection{City in State}
\label{sec:sub_sec_city_in_state}
This group of questions try evaluate the semantic relationship between a city
and a state for cities in the United States of America.  However, the
objective of the experiment is to have a standard,  almost
language independent comparable test set for the word vectors generated by \textit{Word2vec}, tasks that
are centered in information mostly found in a specific language are not relevant and
will not provide good information about the quality of the vectors. For this
reason the complete tasks group was removed from the dataset. An alternative could have been to replace it with a similar
tasks (e.g. using cities in Germany / Austria). In this case however, a
comparison with another language would have been impossible as the
information sources might not contain the  information required or even the words
themselves.

\subsection{Man-Woman}
\label{sec:sub_sec_man_woman}
This tasks try to evaluate the semantic relationship between words associated
with men and women. For this tasks the equivalent German version of the word
were used. For validation of the translation  a online German - English
dictionary was used.

\subsection{Adjective to Adverb}
\label{sec:sub_sec_adjetive_adverb}
This task models the relationship between an adjective and its correspondent
adverb. However in German this task cannot be applied,  because unlike English,
German does not make a distinction in form between a predicate adjective and
an adverb \cite{durrell2011hammer}. For example in English:

\begin{itemize}
\item The man is quiet. (quiet = adjective)
\item The man sings quietly. (quietly = adverb)
\end{itemize}

Whereas a German speaker would say:

\begin{itemize}
\item Der Mann ist leise. (leise = adjective)
\item Der Mann singt leise. (leise = adverb)
\end{itemize}

For this reason the trained word vector will not be able to differentiate the
roles. Thus, this tasks is removed from the set. 

\subsection{Opposite}
\label{sec:sub_sec_opposite}
This task try to model the \emph{opposite} semantic relationship between words.
Although it may seem relatively easy to adapt this task to German there
things that need to bee taken into account. In German,  not necessarily the
most common ``antonym'' is used nor they have a regular morphological
structure as in English. For example  in English is usual to form the
opposite by adding the prefix \textit{in} ,  \textit{un} , \textit{im} or
\textit{dis}. In fact, all of the word pairs that appear in the English
version of this set are formed in such way. This is not always true in
German. For example, the opposite for the word  \textit{\"{u}berzeugend} (roughly
translating to \emph{convincing} in English) has not opposite in that form.
The only way to form the opposite is by adding the negation, e.g.
\textit{nicht  \"{u}berzeugend }. Another example would be the pair
\textit{geschmackvoll} - \textit{geschmacklos}, which are as well opposites,
but where the suffix is the difference.
However, most of the word in these tasks could be also build in a similar
fashion as in the English set, therefore we adapt this task by looking for
the best translations that can be formed with that common structure.

\subsection{Comparative and Superlative}
\label{sec:sub_sec_comparative_sup}

The comparative task  try to model to evaluate the syntactic relationship between a word
and its comparative form, for example the word pairs  \textit{young} and
\textit{younger}. Respectively, the superlative task aims to evaluate the
superlative relationship  (e.g. \textit{safe}  and \textit{safest}). 

Many of the words belonging to this dataset can be easily translated to
German directly. However, there are some caveats. First, English have many
more words for representing the same semantic concept than German.  English is language from
the family of Germanic languages, therefore shares the common structure
and grammar. However, many words have been borrowed from Latin, expanding the
vocabulary  and in many cases replacing the words from Germanic origin. 
For example, the word \textit{tall}  comes from Old Germanic and  the
word \textit{large} comes from Latin. Both words could be translated to
German as \textit{hoch}. However, the English word  \textit{high} of German
origin can also be translated as \textit{hoch}.
% Another example of this is the German the word  \textit{gross}, that represents
% the quality  of being  \textit{big}, but also \textit{large} or \textit{tall}.
That is a unique concept in German can represent many things in English. 
Give this, only one word is maintained in German and one English representing
the same concept making the number of word pairs equivalent.


% To measure quality of the word vectors, we  a comprehensive test set that contains ﬁve types
% of semantic questions, and nine types of syntactic questions. Two examples from each category are
%
\subsection{Present Participle}
\label{sec:sub_sec_present_participle}

This tasks try to model the relationship between verbs and its participle
(e.g. \textit{come} -\textit{coming}). In English is used the participle in
much more ways than German: 

\begin{itemize}
\item  To form the progressive (continuous) aspect: Jim was sleeping. as an
  adjective phrase modifying a noun phrase: The man sitting over there is my
  uncle.
\item adverbially, the subject being understood to be the same as that
  of the main clause: Looking at the plans, I gradually came to see where the
  problem lay.
\item  similarly, but with a different subject, placed before the
  participle (the nominative absolute construction): He and I having
  reconciled our differences, the project then proceeded smoothly.
\item more
  generally as a clause or sentence modifier: Broadly speaking, the project
  was successful.

\end{itemize}

In German, unlike English  the present participle is used almost exclusively as an
adjective or adverb and as such are inflected. This is imply that in a model
such as  \textit{Word2vec} the same word after the declension will be
represented with different words. For the purpose of this tasks, this mean
the is expected to have low number of correct answers.

\subsection{Nationality Adjective and  Plural nouns}
\label{sec:sub_sec_nat_plu}

The nationality adjective tasks model the relationship between a country and
the the adjective representing the nationality. For example, for the word
\textit{Colombia}, the respective adjective is \textit{Colombian}. In German,
the nationality is also an adjective, and as such, it  is inflected based on the
case and genus of the substantive.  For example  \textit{Kolumbianisch} may appear 
inflected as \textit{Kolumbianischer},  \textit{Kolumbianischem},
\textit{Kolumbianische}, etc.  Similar to the participle, many words will
represent the same concept, thus low correct answer rate are expected in this
task.  
The plural nouns task model the relationship of a word and its plural the
proportion of words that are the same plural is higher than English. This is
a factor that will affect the test results because the same word is never taken as a
valid answer. 


\subsection{Past Tense and Plural Verbs}
\label{sec:sub_sec_plural_verbs}
The two task find syntactic relationship between verbs and they past tense
and the conjugation respectively. This relationships does not hold in German,
and therefore are removed from the test.

\section{Summary of Changes} 
Table \ref{tab:summary_of_changes_task} shows a summary of the changes
performed in other to generate a set for the German language.

 \begin{table}[h]
   \centering
   \caption{Summary of changes to the original task } 
   \label{tab:summary_of_changes_task}
   \small
   \begin{tabular}{ |l|l|  }

   \hline           
   Task  & Action   \\  \hline           
   Common capital city & Translated \\
   All capital cities  & Translated \\
   Currency & Translated  \\ 
   City-in-state   &  Removed - Language dependent \\
   Currency & Translated  \\
   Man-Woman  & Translated  \\
   Adjective to adverb  &  Removed - do not apply in German \\
   Comparative & Translated - Some words removed due to polisemy \\ 
   Superlative & Translated - particle \textit{am} not used \\
   Opposite  & Translated \\
   Present participle & Translated \\
   Plural Nouns  & Translated \\
   Past verbs &  Removed \\

   Nationality adjective & Translated \\
   
\hline

  
\end{tabular}
\end{table}
    
In parallel to this changes, the English version was also modified to match
this new set of tasks so a comparison could be performed in a somewhat
equivalent manner.  Table show sample 


\begin{table}[h]
  \centering
  \caption{Examples of word pair  of task  used  to evaluate the German word
    vector model }
  \label{tab:task_deutsch_modified}

  \begin{tabular}{ |l| |c|*{4}{c| |c| c | c }  }

  \hline           
  Type of Relationship &  \multicolumn{2}{c||}{Word Pair 1} &
  \multicolumn{2}{c|}{Word Pair 2} \\  \hline           
  Common capital city & Bern &  Schweiz & Kairo  & Aegypten \\ 
  All capital cities  & Zagreb &  Kroatien & Aschgabat & Turkmenistan  \\
  Currency & Argentinien & peso  & Lettland  & lats \\ 
  Family & Bruder & Schwester  & Grossvater & Grossmutter \\
  Man-Woman & Koenig & Koenigin  &  Prinz & Prinzessin   \\  \hline  
  Opposite & zulaessig & unzulaessig & moeglich & unmoeglich \\  
  Comparative & billig & billiger & warm & waermer \\  
  Superlative & schlecht & schlechtesten & gut & besten \\  
  Present participle & fliegen & fliegend & laufen & laufend \\  
  Nationality adjective & Weissrussland & weissrussisch & Frankreich  & franzoesisch  \\  
  Plural nouns  & Banane & Bananen & Loewe & Loewen \\
  \hline
\end{tabular}
\end{table}



\section{Description of Experiments}
\label{sec:german_eng_experiments}

this section described the experimental set-up used to evaluate the German
Word2vec model. The objective of this experiments is two answer several
question regarding the performance of Word2vec the applicability of Word2vec
on languages different than in English and in particular for German.  In
particular:

\begin{itemize}
\item What is the performance of the German language word vector model in comparison to that of
  the English language?
\item Given the morphological differences and complexity of languages such
  a German, is Word2vec able to capture the information properly?
\item Is there any preprocessing of the training data  that might help to capture word vector
  (e.g. Stemming)?
\end{itemize}

In order to answer these questions the following tasks are performed:

\begin{itemize}
\item The training of several Word2vec models using the Wikipedia German
  database.
\item Training of several several Word2vec models using Wikipedia English as
  training  database.
\item Comparison of the performance of the models in the modified task
  described in section \ref{sec:adapt_task_german_lang}.
\end{itemize}


\subsection{Dataset}
\label{experiments:sub:dataset}

In the original paper \cite{DBLP:journals/corr/abs-1301-3781} the author
trained the word vectors using Google News internal dataset containing about
6 billions of tokens. This dataset is not available publicly. The authors have made
available the pre-trained 300-dimensional  word vector  using Google internal
dataset containing 3 million words and phrases.

The closest open dataset available with such variety is the Wikipedia, and
we use both, the English and the German Wikipedia for our experiments.  It is
important to notice, that as it happens with most of the information
available in World Wide Web, The majority of the available information on the
Web is in English, and as discussed before, the amount of data available affects the quality  of the trained
word embeddings. 

For the English and German word vector model  the Wikipedia is used. For
German the complete article dump of the 24$^{th}$ of October of 2013  of the
Wikipedia is used.  For English, the dump of all the articles from the
4$^{th}$ of November of 2013 is used. The dates are reported for informative
purpose. The dataset can be obtained from the Wikipedia
servers.\footnote{\url{http://dumps.wikimedia.org/}}.

As this dump are in XML and contains characters and words that are not of
interest when building the word vector representation the following clean up
procedure was applied to them:

\begin{itemize}
\item Remove all XML/HTML tags from the file preserving text and caption for
 URLs and images.
\item Remove special characters such as \&,  [, \}, etc.
\item make all characters lowercase.
\item spell out numbers.
\item remove any other character that is not alphanumeric.
\end{itemize}

In addition to this, for the German language the additional modification were
performed
\begin{itemize}
\item Transform \textit{umlauts} to the ASCII spelling.  e.g. \"{o} is
  transformed to \textit{oe},  \"{u} is transformed to \textit{ue}  and so
  on. 
\item The \textit{Eszett} or \textit{scharfes S} (Symbol '\ss') \  is transformed to ss.
\end{itemize}


To perform this changes a modified version of a Perl script originally
designed to perform this tasks (for the English Wikipedia) was used
\footnote{\url{http://mattmahoney.net/dc/textdata.html}}. This script is
linked on Word2vec's Google code project site.

Table shows \ref{tab:summ_dataset_germanword2vec} the datasets used
to train the file the word vectors with their size and number of tokens after
the preprocessing. 
  

 \begin{table}[h]

   \centering
   
   \caption{Summary of datasets} 
   \label{tab:summ_dataset_germanword2vec}
   \small
   \begin{tabular}{ |l|c|c| }
   \hline           
    File Name &  Size (GB) & Number of tokens  \\  \hline           
    dewiki-20131024-pages-articles.txt & 6.5 &   1051584822 \\ 
    enwiki-20131104-pages-articles.txt & 18 &  3185913717  \\
    \hline

\end{tabular}
\end{table}

  

We can see that the English dataset contains almost 3 times as tokens as the
German one. This mean that for the English word vector model  there much more
sentences to train form, and therefore can be expected a better performance
from this model in the tasks described before. 
   
\subsection{Model Training}
\label{experiments:sub:Training}

There are many parameters that can be modified in order to adjust the model
and have it performs better on both the  semantic and syntactic tasks. The
selection of this parameters affect not only the accuracy of the model but
also the training speed.  Table \ref{tab:word2_vec_parameters} shows and
overview of the most important parameters with short description. For a
complete description of each of them please refer to section [LINK TO SECTION]. 


\begin{table}[h]
   \centering
   \caption{Summary model training parameters} 
   \label{tab:word2_vec_parameters}
   
   \small
   \begin{tabular}{ |l|c|p{5cm}| }
   \hline           
    Parameter &  Possible Values & Comment \\  \hline           
    \multirow{2}{*}{Architecture}  & skip-gram  & Slower, better for frequent
    words. \\ 
    \cline{2-3}
    & \ac{CBOW}  &  Faster than skip-gram. \\ \hline
    \multirow{2}{*}{Training Algorithm}  & \ac{HS}  & Better for infrequent words.   \\ 
    \cline{2-3}
    & Negative Sampling & Better for frequent works \\ \hline
    Sub-sampling  & 1e-3 to 1e-10  &  It  can improve both accuracy and speed for large data
    sets Useful values are in range \\ \hline
    Dimensionality  & 10-640 & Higher dimension the better results. 
    it depends on data availability.  \\ \hline
    Context (Window size)  & 5 - 15 & The number of word used to predict. \\ \hline

    
\end{tabular}
\end{table}

For training  the model the public \emph{C} implementation available of
\textit{Word2vec} \footnote{\url{https://code.google.com/p/word2vec/}} was
used.  This is a highly optimized C implementation. it was slightly modified to account for the different
number to task categories and to provide more debugging information useful
for the model training. 

In a similar fashion as in the original paper
\cite{DBLP:journals/corr/abs-1301-3781}, in order to quickly train and evaluate the models
only the 30.000 most common words of the dataset are used to evaluate the tasks. 
% The skip-gram architecture worked
%better in this evaluation

\subsection{Evaluation}
\label{experiments:sub:evaluation}

As in the original article describes, the overall accuracy for all question
types is calculated as well as  for each question type separately (semantic,
syntactic). A question is then assumed to be answered right  only if the
closest word to the vector computed using  is exactly the same as the correct word in the
question. 
 
\section{Empirical Results}
\label{sec:sub_empirical_results}

As mentioned before, with the purpose of obtaining  a quick evaluation of the
model, only a subset of the dataset was evaluated it with the 30.000 
most common words on it. The German syntactic and semantic task described in
section \ref{sec:task_description} is used  to assess the quality of the
model. 


The four possible combinations of architecture and models were evaluated
quickly using this approach. For that a dimensionality of 300 was chosen with
a fixed window of 10. Table \ref{tab:initial_w2v_comparison} shows the result
of this tests. As it can be seen, the best training parameter combination is
Skip-gram for the architecture with Negative Sampling.

% Initially, in order to train faster the
% models  the \ac{CBOW} architecture was used. However, when evaluating the
% models trained with this architecture the  accuracy was close to 0
% regardless of the data size. Consequently the alternative architecture,
% Skip-gram was used.







\begin{table}[h]
\centering
\caption{Comparison of the combination of the available architectures and algorithms.
 Only questions containing words form the most  frequent 30k are used.} 
\label{tab:initial_w2v_comparison}


\begin{center}
\small

\begin{tabular}{|l|l|c|c|c|}
\hline
 Model Architecture  &   Algorithm  &  Semantic Accuracy  &  Syntactic Accuracy  &  Total Accuracy  \\
\hline
 CBOW               &  HS                   &              76.65  &               31.73  &           56.59  \\
 CBOW               &  NEG                  &              81.17  &               50.26  &           65.73  \\
 Skip-gram          &  HS                   &              77.59  &               34.02  &           55.82  \\
 Skip-gram          &  NEG                  &              84.40  &               48.58  &           66.54  \\
\hline
\end{tabular}
\end{center}

\end{table}


After evaluating different architectures, the  model was evaluated with
different  training data sizes and dimensionality. Table
\ref{tab:initial_w2v_training}  shows the results of this training. The main
difference in comparison with the original work is that we use directly
Negative Sampling  instead of \ac{HS} for the training algorithm. 


From table \ref{tab:initial_w2v_training} can be also noticed that if 
neither the dimensionality of the word vectors or the number of
training words alone can improve the performance of the word vectors. Only the
combination of an increasing amount of both will improve the accuracy. In
fact, as can be seen using high dimension with a small amount of training
words might even hurt performance.


\begin{table}[h]
\centering
\caption{Accuracy on subset of the semantic task Word relationships test set using
vector Skip-gram architecture with Negative Sampling. Only questions containing words form the most
 frequent 30K are used. } 
\label{tab:initial_w2v_training}

\small
\begin{tabular}{|c|cccccc|}
\hline
 Dimensionality / Training Words  &   24M  &    49M  &    98M  &   196M  &   392M  &   780M  \\
\hline
                              50  &  6.84  &  17.51  &  33.95  &  42.61  &  43.49  &  49.02  \\
                             100  &  6.84  &  18.61  &  38.64  &  50.30  &  56.97  &  57.90  \\
                             300  &  5.57  &  18.22  &  37.05  &  52.71  &  63.77  &  77.33  \\
                             600  &  4.85  &  17.40  &  34.02  &  49.01  &
                             61.80  &  66.92  \\
\hline
\end{tabular}
\end{table}


After performing the previous experiments, it was visible that the best
architecture and training algorithm were Skip-gram and Negative Sampling
respectively. With this two parameters we evaluated various combinations of
other model with the full data, evaluated initially again on the 30.000 most
common words  and then in the full dataset of questions. The subsampling
parameter was obtained empirically from trying distinct values of it. As it
happens, it also matches the one obtained for the English language in the
original work \cite{DBLP:journals/corr/abs-1301-3781}.

Table \ref{tab:main_results} shows  overview of the best models obtained from
the complete German Wikipedia dataset  and their performance on the full set
of questions. In contrast with the original work
\cite{MikolovSCCD13} \cite{DBLP:journals/corr/abs-1301-3781}, the best model
has only 300 dimensions. This is caused by the fact the models were trained
with considerable less training data than Google News dataset. Nonetheless,
the dimension is higher than the usual ones found in other word vector embedding models
trained using similar techniques and similar datasets.\cite{Turian:2010:WRS:1858681.1858721} \cite{DBLP:journals/corr/abs-1103-0398}.

\begin{table}[h]
\centering

\caption{Accuracy of the best models obtained using the Skip-gram
  architecture with Negative Sample on the German Wikipedia dataset. A
  subsampling of $10^{-5}$ is used} 
\label{tab:main_results}



%TODO Elaborate on this

\small
\begin{tabular}{|l|c|c|cc|c|}
\hline
Method  &  Win.  &  Dim   &  Syntactic [\%] & Semantic
[\%] &  Total accuracy [\%]  \\
\hline

NEG-10  &   5  &  300  &  59.4  &  25.3  &  42.3  \\
 NEG-10  &   5  &  400  &  57.5  &  25.6  &  41.5  \\
 NEG-10  &   5  &  500  &  54.3  &  24.3  &  39.2  \\
 NEG-15  &  10  &  300  &  63.0  &  24.9  &  \textbf{44.0}  \\
 NEG-15  &  10  &  400  &  58.9  &  26.2  &  42.6  \\
 NEG-15  &  10  &  500  &  60.1  &  24.3  &  42.3  \\
\hline
\end{tabular}
\end{table}

The second column of table \ref{tab:language_comparisson} shows the result of
the best German model separated by category. The second column The fourth column
shows the performance of pre-trained word vectors on the Google News data
set. Next section will discuss the relationship between these columns in more detail.

We can note that section  \textit{currency} from
the semantic dataset; the \textit{present participle}  and 
\textit{superlative}  sections are the one with the  worst performance. This
result can be explained for many factors, being the dataset the most
important. Both in amount of data and variety. However, from a exploration of
the model it can be seen that the concept of currency is maintained although
the exact semantic relationship is not built.  For example, taking one of the
tasks:  $(\text{'Algerien','dinar'},\text{'Angola','kwanza'})$. If we define
$x$ as the result of applying:  $x = vector(\text{dinar}) -
vector(\text{Algerien}) + vector(\text{Angola})$, then in order for this task
to be answered correctly, the closest vector to should be $vector(\text{kwanza})$.
However the top-5 closet vectors are:


\begin{center}
\small
\begin{tabular}{|l|c|}
 \hline
 Word             &  Cosine similarity to $x$  \\
\hline
 \textit{peso}           &      0.6193921566009521  \\
 \textit{centavos}       &      0.6159522533416748  \\
 \textbf{kwanza}         &      0.6066386699676514  \\
 \textit{kwacha}         &      0.6008095741271973  \\
 \textit{waehrung}       &      0.6000255942344666  \\
\hline
\end{tabular}
\end{center}


Although the closest vector is not the expected one, all the words are related
somehow with the concept of \textit{money} or \textit{currency}.  For example
the word \textit{peso} is the currency of many Latin American countries. In
fact with some additional modification  to they vector operations it is
possible to obtain the desired answer. For example if we define $x$ as  $x = vector(\text{dinar}) -
vector(\text{Algerien}) + vector(\text{Angola})  + vector(\text{Afrika}) $,
that is, the same operation as before but adding the vector representation of
the word \textit{Afrika}, it produces a vector with the following top-5 closest
vectors:


\begin{center}
\small
\begin{tabular}{|l|c|}
 \hline
 Word                 &  Cosine similarity to $x$  \\
\hline
\textbf{kwanza}        &        0.7224324941635132  \\
 \emph{angolas}       &        0.6794506311416626  \\
 \emph{angolanische}  &        0.6792401075363159  \\
 \emph{kwacha}        &        0.6728493571281433  \\
 \emph{metical}       &        0.6719954609870911  \\
\hline
\end{tabular}
\end{center}

Which would return the correct answer for the task. This does not work all
the cases, but it shows that the semantic relationship is built albeit not
maintaining the exact relation between words as the tasks expect. In the
other cases  a similar phenomena occurs: the right answer is in the
top-5 or top-10 of closest word representation  of  the resulting vector. This suggests 
that with more training data it is possible to build better word
representations  that fulfill the relationships defined  by the analogical
reasoning task.



\begin{table}[h]
\centering
\small
\caption{Accuracy of the best models separated by category using the German
  Wikipedia, the English Wikipedia and the pre-trained Google News dataset
  word vectors. } 
\label{tab:language_comparisson}


\small
\begin{tabular}{|l|c|c|c|}
\hline
 Category                  &  Accuracy DE [\%]  &  Accuracy EN [\%]  &  Accuracy Pre [\%]  \\
\hline
 capital common countries  &             89.33  &             96.05  &              83.20  \\
 capital world             &             72.33  &             88.68  &              79.13  \\
 currency                  &              7.97  &             13.63  &              27.37  \\
 family                    &             47.62  &             79.64  &              84.58  \\
\hline
 Total Semantic            &             63.12  &             78.40  &              72.88  \\
\hline
 opposite                  &             14.67  &             33.62  &              42.73  \\
 comparative               &             33.06  &             75.83  &              90.84  \\
 superlative               &              3.69  &             28.25  &              87.34  \\
 present-participle        &              3.17  &             60.61  &              78.22  \\
 plural                    &             36.79  &             70.20  &              86.04  \\
 nationality adjective     &             33.33  &             89.31  &              89.93  \\
\hline
 Total Syntatic            &             24.83  &             63.46  &              81.99  \\
\hline
 Total                     &             44.02  &             70.47  &              77.73  \\
\hline
\end{tabular}

\end{table}





\subsection{Comparing German and English Word Vectors}
\label{sec:sub:comparing_english_german_w2v}

In this section models trained using the English language Wikipedia and the
German Wikipedia  are compared. It is clear that from the linguistic
point of view it does not make sense to compare two language models for
different languages and in that same idea word vector representations. 
However, the purpose of this section is to assess to the extend of the
possibilities, the behavior the word vector model on languages with different
degrees of syntactic complexity. For that, we measure the quality of the word
vector obtained from a  German language  dataset compared to the ones trained
with an  English corpus. Namely the Wikipedia.

Although in section \ref{sec:adapt_task_german_lang} we described the
creation of a  equivalent analogical reasoning task for the German language,
obtaining  a unbiased comparison between vector models from different
languages is a hard task.  First there is not an equivalent dataset for
German language.  As we described above, the Wikipedia raw text is used to
train the word vectors. However, the English Wikipedia contains much more
text than the German Wikipedia, triplicating it both in size and  number of words.

In addition to this, the original data, namely the Google News dataset, on
which the original word vectors were trained, is not available
\cite{DBLP:journals/corr/abs-1301-3781}. However, some pre-trained word
vectors were made available by the authors \footnote{\url{https://code.google.com/p/word2vec/}}, and
are used reference to compare the two vector models trained from Wikipedia.
These word vectors were trained with about billion of words and phrases.

% Even with these limitations,  it is still valuable to explore the behavior of these word
% vectors trained in different language dataset as well as to explore the
% difference in performance compared to pre-trained vectors.  

Table \ref{tab:language_comparisson} compares the best model for each language in each
category. Both German and English behave fairly similar per category, giving
the impression that the model work in a similar manner in German language.
However for the syntactic task  the performance is  different when
compared with other languages. In particular for the superlative and the
present-participle  the performance is low compared to other categories in
German and with the  equivalent categories for English. This may be caused by
the complex and different German syntax and usage. The data size is also an
important factor that affects the performance.  We can see that when we
compare the dataset of English trained on the Wikipedia and the one  pre-trained. 


Although the pre-trained vectors are trained with much more data
than the Wikipedia ones, for some tasks the performance is better for the
later. That suggest that is not only the size but the \textit{quality} of the
text might have important effect on the performance. Quality in these context
means if the text contains the semantic or syntactic relationships  that the
test set expects. In other words, less data contained better semantic or
syntactic relationship might be better for training that a lot of text with
no context. Another example of this will be shown in chapter [PUT REFERENCE
HERE] word vectors from Wikipedia are used to perform document classification.

%- TODO: Mention that the dataset on which the trained vectors are obtained
%from affects the end result (so not only syntax is learned but also semantics)
%- TODO: Less data with better relationships between entities might be even
%better than a lot of data with no context.

% \textbf{TO MENTION}
% Regarding the results is obvious that are less  the available word vectors are in English and the original data used  to train
% them is not available.  In addition to this, the sources that might be
% equivalent for the German language.  Therefore, these results cannot be
% compared with the one reported in the original article. also previous work
% compares the model in similar tasks and word2vec showed better perforance
% therefore we are no using other models  with German lanaguage is limited so we cannot really have a inter-model
% comparisson.

% Add a table here were I compare models with different architectures and
% different: CBOW, Skipgram and Neg. with diffeerent (Size, training words, and
% training time) and split it into semantic, syntantic and Total. (From Google
% tabel and check it out) - Time in (min) (with and wigthout Subsampling of 3
% of Four)
% ANALOGICAL REASONING  use that on the test %


%TODO: Add a table in which we compare words and their tranlation with the
%more similar words 
\subsection{Effect of Preprocessing on the learned word vectors}
\label{sec:sub:effects_of_preprocessing_on_learned_word_vectors}

One of the driving ideas behind of feature learning is to learn useful
representations with no prior knowledge and with minimum human
involvement. However, as the same authors of \textit{Word2vec} state, it is likely
that adding information to the model about morphological relationship of
words, and in general any additional relevant prior knowledge,  will produce better
representations.  One way of indirectly adding such information to the model
without modifying its formulation is to perform preprocessing on the training
dataset. This chapter explores how preprocessing, in particular stemming,  affects the
quality of the word vectors. 
 

In many \ac{NLP} applications, in particular, \ac{IR} related tasks,  text preprocessing is a common, if not
necessary, step to obtain good results. Given highly inflective nature of
German is natural to expect improvement in
certain analogical reasoning tasks.  One of the most common preprocessing steps in NLP is stemming (i.e. grouping
words that share the morphological root). Stemming has proven successful in
the field of \ac{TC} \cite{Sebastiani02}. For languages such as  German were the nouns and
adjectives are also inflected it might be helpful to reduce the vocabulary
size.  

To evaluate the effect the stemming on word vector quality, different
stemming approaches based on the Porter stemmer \cite{Porter80} were applied  to the original Wikipedia
German text corpus described in section \ref{experiments:sub:dataset}.

\begin{itemize} 
\item Partial stemming algorithm using only the step\_1 suffixes of the
  stemmer algorithm.
\item Application of the complete Potter stemmer algorithm.
\item Remove the declensions of words ending in  \textit{sch}  (i.e. \textit{sche}, \textit{scher},
\textit{schem}, etc.)  and replace it simply by \textit{sch}.
\end{itemize} 



\begin{table}[h]
\centering
\small
\caption{Comparison of different stemming schemes. The model were trained
  with the best parameter after stemming the original file using the
  strategies described in section \ref{sec:sub:effects_of_preprocessing_on_learned_word_vectors}. } 
\label{tab:stemming_comparisson}
\small
\begin{tabular}{|l|c|c|c|}
\hline
 Category                  &  Custom Stemming [\%]  &  Full Stemming [\%]  &  Replaced \emph{sch} [\%]  \\
\hline
 capital common countries  &                 56.92  &               38.53  &                     91.11  \\
 capital world             &                 39.00  &               10.19  &                     73.21  \\
 currency                  &                  5.77  &                0.11  &                      7.62  \\
 family                    &                  6.06  &                5.19  &                     48.92  \\
\hline
 Total Semantic            &                 33.94  &               10.71  &                     63.94  \\
\hline
 opposite                  &                 13.33  &                2.66  &                     15.33  \\
 comparative               &                 25.22  &                0.00  &                     34.69  \\
 superlative               &                  0.00  &                0.00  &                      4.06  \\
 present-participle        &                  1.48  &                0.00  &                      2.56  \\
 plural                    &                  1.81  &                0.45  &                     34.37  \\
 nationality adjective     &                 77.19  &                1.51  &            \textbf{90.06}  \\
\hline
 Total Syntactic            &                 25.66  &                0.71  &                     39.38  \\
\hline
 Total                     &                 29.82  &                5.74  &                     51.82  \\
\hline
 Questions seen            &                 67.73  &                7.71  &                     94.22  \\
\hline
\end{tabular}
\end{table}


All the stemming process was performed using the popular Python library for
\ac{NLP}  \ac{NLTK} \cite{BirdKleinLoper09}. The
description of the stemmer algorithm for the German language can be found
online\footnote{\url{http://snowball.tartarus.org/algorithms/german/stemmer.html}}.

The first modification aims to remove the plural (already affecting the
outcome of the syntactic task) and the declension of adjectives and nouns.
The second modification runs a full Potter stemmer algorithm. The last was an
additional experiment aimed to verify that performing some modification to the
training dataset would allow the model to perform better in certain task.

Note that all modifications, although probably helping in some of the tasks
would undoubtedly incur in loss of performance in another. For examples, if
some adjectives are stemmed, the ability of recognizing comparatives or
superlative are lost.

Table \ref{tab:stemming_comparisson} show the results of training the word vectors
using the  best parameters on this modified dataset.  The rows show the
performance per section of the set of tasks. The \textit{Question Seen} row
shows the amount of question that appear.
As it can bee seen
both of the stemmings based on the Porter stemmer hurt the performance of the
of the classifier. In particular the full stemming causes the model to only
achieve as little as 5.74\% accuracy and only seeing 7.71\% of the total
question. Surprisingly enough the preprocessing that consisted of removing
the declension of adjectives  ending in \textit{sch} not only
improved the target category, \textit{nationality adjective} that went from
33.33\% to 90.06\% accuracy but also slightly improved the accuracy of other
categories by losing less of 2\% of question seen. In other words, some
well-chosen preprocessing  step helped to improve the performance not only
for specific tasks but in general over the whole task set.

To conclude, the results shown that although the traditional approach to text
preprocessing for IR will not work to improve word vector quality in the
context of these semantic and syntactic tasks,  some hand picked
preprocessing might indeed improve the  model accuracy indirectly providing
the algorithm with morphological information  of words.
% as expected by the authors of \textit{Word2vec} had a positive effect on the accuracy \cite{DBLP:journals/corr/abs-1301-3781}.


% TODO: Table comparing the things


%\subsection{Regularities of the German Language Captured by the model}
%\label{sec:sub:particularities_of_the_German_language}

\section{Conclusion}
\label{sec:conclusion_and_future_work}   

This chapter explored the performance of \textit{Word2vec} model in a German
language. To evaluate the quality of the word embeddings generated a  similar
albeit reduced logical reasoning tasks was constructed  based on the original
for the English language that was used in the original work. The result show
that the model performs well in German, although the complexity of the
language causes certain task to under perform when compared to the equivalent
task in English. Another factor affecting the performance of the model is the
amount of data available in German compared to the one used for English, but
the Wikipedia set and the not publicly available Google News dataset.

Another important outcome of this  empirical exploration, is that the specifics
of the text affect the quality of the word vectors.  In other words, the
relationship of the text and the objective task is important.  For example,
when comparing the English word vectors training solely with Wikipedia vs the
ones trained with Google News dataset, some of the tasks performed better
with Wikipedia trained vector  although the amount of text the were trained
from is much less than the Google News dataset. This mean also that for
specific tasks a large amount of text might not be necessary to get good word
vector representation.

Finally,  some  preprocessing approaches were applied to the Wikipedia
corpus to evaluate whether this approaches helped to improve the quality of
the word representations. However none of the traditional approaches helped.
The heuristic one, based solely on stemming specific suffixes coming from
declension not only improved  3x times the performance on a specific tasks
but also in general on the overall tasks. This suggest that specific
handpicked approaches to preprocessing, oriented to specific tasks  might improve even more the quality of
vectors.

% Quality of the text.
% Lack of data
% Preprocessing 

%TODO: Do an experiments in which check if the words that are usually put
%together  can be infered by summing and resting stuff 
% LIKE - %Zusammenabrechung?


%TODO: Ask christoph.
%%%%% 
%%% %TODO: Declination in German additonal task.
%%%% Inflection of der die das
%%%% Inflrection of adjectives



%% • private datasets that do not allow experiments to be repeated are used
%% • ad hoc preprocessing is used that favours the proposed technique, or completely
%% artificial datasets are used
%% • comparison to proper baseline is completely missing
%% • baseline technique is not tuned for the best performance
%% • in comparison, it is falsely claimed that technique X is the state of the art (where X is usually n-gram model)
%%  possible comparison to other advanced techniques is done poorly, by citing results achieved on different datasets, or simply by falsely claiming that the other techniques are too complex 

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../main.tex"
%%% End:
%  LocalWords:  dimensionality

\chapter{Empirical Study of Word Vectors for the German Language}
\label{chapter:wor2vec_german}

%% In this chapter 

%% - What we do in this chapter 

%% - Rationale why we do that how we do it and why

%% - Short descrioption of Experiments

%% Tasks for English and German and comparisson of Gramatical and semantical differences

%% Datasizes and so on

%% - Embedding using t-SNE of German wikipedia (top X)

%% - Embedding using t-SNE of GiniData using top X

%% - Table of the experiments

%% - Analysis of the results 

The previous chapters introduced the concepts and related work  in the fields
of  \ac{NLP}, \ac{NNLM} and vector representation of words. This chapter will
focus on the application of a particular model, namely \textit{Word2Vec}, to
the German language.  

% There art two main reasons that justify the study of the behavior of word
% vector representation in a language as morphologically rich and  inflective
% as German.

Although that  evidence that word vector representation might work
well in other languages other than English has been presented
\cite{DBLP:journals/corr/MikolovLS13}, to the date  there are not in depth studies or empirical evaluations of the
performance of word representation for such languages,  in similar or equivalent tasks as the
ones performed by \cite{DBLP:journals/corr/abs-1301-3781} for the English
language. 

Previous work has shown that  n-gram backoff language models, the state of the art
technique, does not perform  as well for inflectional  languages as they work
for English \cite{conf/icassp/MikolovKBGC09}.  Previous work also shows that
\ac{NNLM} captures  better the complexity of such languages
\cite{conf/icassp/MikolovKBGC09}\cite{DBLP:journals/corr/MikolovLS13},
therefore  is possible that word vectors generated by such model  will 
improve  and simply many \ac{NLP} applications in
these languages as they have been shown to do in
English \cite{collobert:2008} \cite{Turian:2010:WRS:1858681.1858721}.  This makes therefore an initial evaluation of the quality of the word vector
attractive.

The rest of the chapter is divided as follows: The first part described the
selected tasks, the rationale behind the adaption made to fit the German
language and the data set  and the different preprocessing alternatives we
tried. The second part described the training approach as well describe the most
important result of our approach.

%  In this chapter we will
% describe a set of tasks adapted from \cite{DBLP:journals/corr/abs-1301-3781}
% to better match the characteristics of German language. In addition we will
% compare  the performance with English language.


%  As word vectors are generated
% by \ac{NNLM} \cite{Bengio:2003:NPL:944919.944966}, it is expected therefore,
% t

% First  as reported in previous work, n-gram backoff language models, the state of the art
% technique, do not perform  as well for inflectional  languages as they work
% for English \cite{conf/icassp/MikolovKBGC09}. Recent alternatives
% n-gram based models are \ac{NNLM} \cite{Bengio:2003:NPL:944919.944966} described
% in detail in CHAPTER [TODO:REF]. Besides the language model,  \ac{NNLM}
% generate the word  vector representations .

% in this type of languages \cite{conf/icassp/MikolovKBGC09}\cite{DBLP:journals/corr/MikolovLS13}.
% Second, 

% As mentioned before,  vector representations have shown the ability of
% improve existing \ac{NLP} taks 

% -  There is no studying comparing performances in two different languages and
% how the differences between the language and the standard language affects. 



%  [Thurian and
% Other people papers listed on Mikolov]

% - This work have done in english (not studied in depth in high inflectional
% languages) however there are not study which theoritically explains what 

% - There are some evidence of good performance morphologucally rich (highly
% inflective) lanaguage 

% %
%- [On the other hand, n-gram backoff models do not work
% as well for inﬂectional languages as they work for
% English] \cite{conf/icassp/MikolovKBGC09} 

%  Several alternative model has been proposed based on \ac{NNLM}

% - As the word embeddings are based also en \ac{NNLM} it is expected that
% thereofore works better. 

% - We use in our work German is a morpholgically rich and moderate inflected
% language (CITATION). We want to test evaluate the performance of this
% particular word embeddings. 

% Similar to the English study we define a set of tasks 

% To evaluate a language model is necessary define a set of tasks 
% For that purpose, a equivalent subset of tasks targeted at the German language were
% defined in the same maner as by Mikolov et al.
% \cite{DBLP:journals/corr/abs-1301-3781} do for the English.  


% It is challenging to test and compare different language models base just by
% comparing their algorithm description, furthermore if for the language of
% choosing no standard data set exist. For that purpose a equivalent  to the
% one defined by  has been created. 

% This chapter will describe the experiments performed as well as the data sets
% used to test the model. Additionally, a comparison between

% To test this model for  German,  a equivalent set of semantic and
% syntantic tasks were created in a similar way to the ones defined by Mikolov
% et al \cite{MikolovSCCD13}.

% This chapter describes the experiments and and tasks used to measure  

% IMPORTANT ALSO FOR GINI SET - [Neural networks seem to be very useful for tasks where
% we struggle with inflectional languages and limited data
% amounts. While performance of our implementation is not
% high,  presents various optimization techniques that lead to
% significant improvements in required computational time for
% training and lattice re-scoring.] - \cite{conf/icassp/MikolovKBGC09}

\section{Task Description}

In their original paper \cite{DBLP:journals/corr/abs-1301-3781} Mikolov et
al. introduced a comprehensive test that contained five types of semantic
questions. In the original data set  there were 8869 semantic and 10675
syntactic questions. Table BLAH has been 


\begin{tabular}{ |l| |c|*{4}{c| |c| } | }
  \hline           
  Type of Relationship &  \multicolumn{2}{c||}{Word Pair 1} &
  \multicolumn{2}{c|}{Word Pair 2} \\  \hline           
  Common capital city & 1  2 & 3 & 6 & pailander \\ 
  All capital cities  & 1  2 & 3 & 6 & pailander \\
  Currency & 8 & 9 \\  
  City-in-state & 8 & 9 \\  
  Man-Woman & 8 & 9 \\  \hline  
  Adjective to adverb & 8 & 9 \\  
  Opposite & 8 & 9 \\  
  Comparative & 8 & 9 \\  
  Superlative & 8 & 9 \\  
  Present participle & 8 & 9 \\  
  Nationality adjective & 8 & 9 \\  
  Past tense & 8 & 9 \\  
  Plural nouns  & 8 & 9 \\  
  Past verbs & 8 & 9 \\  \hline  
  
    
  

\end{tabular}



% To measure quality of the word vectors, we  a comprehensive test set that contains ﬁve types
% of semantic questions, and nine types of syntactic questions. Two examples from each category are
% shown in Table 1.


%% • private data sets that do not allow experiments to be repeated are used
%% • ad hoc preprocessing is used that favours the proposed technique, or completely
%% artificial data sets are used
%% • comparison to proper baseline is completely missing
%% • baseline technique is not tuned for the best performance
%% • in comparison, it is falsely claimed that technique X is the state of the art (where X is usually n-gram model)
%%  possible comparison to other advanced techniques is done poorly, by citing results achieved on different data sets, or simply by falsely claiming that the other techniques are too complex 

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../main.tex"
%%% End:
\chapter{Empirical Study of Word Vectors for the German Language}
\label{chapter:wor2vec_german}

%% In this chapter 

%% - What we do in this chapter 

%% - Rationale why we do that how we do it and why

%% - Short descrioption of Experiments

%% Tasks for English and German and comparisson of Gramatical and semantical differences

%% Datasizes and so on

%% - Embedding using t-SNE of German wikipedia (top X)

%% - Embedding using t-SNE of GiniData using top X

%% - Table of the experiments

%% - Analysis of the results 

The previous chapters introduced the concepts and related work  in the fields
of  \ac{NLP}, \ac{NNLM} and vector representation of words. This chapter will
focus on the application of a particular model, namely \textit{Word2Vec}, to
the German language.  

% There art two main reasons that justify the study of the behavior of word
% vector representation in a language as morphologically rich and  inflective
% as German.

Although that  evidence that word vector representation might work
well in other languages other than English has been presented
\cite{DBLP:journals/corr/MikolovLS13}, to the date  there are not in depth studies or empirical evaluations of the
performance of word representation for such languages,  in similar or equivalent tasks as the
ones performed by \cite{DBLP:journals/corr/abs-1301-3781} for the English
language. 

Previous work has shown that  n-gram backoff language models, the state of the art
technique, does not perform  as well for inflectional  languages as they work
for English \cite{conf/icassp/MikolovKBGC09}.  Previous work also shows that
\ac{NNLM} captures  better the complexity of such languages
\cite{conf/icassp/MikolovKBGC09}\cite{DBLP:journals/corr/MikolovLS13},
therefore  is possible that word vectors generated by such model  will 
improve  and simply many \ac{NLP} applications in
these languages as they have been shown to do in
English \cite{collobert:2008} \cite{Turian:2010:WRS:1858681.1858721}.  This makes therefore an initial evaluation of the quality of the word vector
attractive.

The rest of the chapter is divided as follows: The first part described the
selected tasks, the rationale behind the adaption made to fit the German
language and the data set  and the different preprocessing alternatives we
tried. The second part described the training approach as well describe the most
important result of our approach.

%  In this chapter we will
% describe a set of tasks adapted from \cite{DBLP:journals/corr/abs-1301-3781}
% to better match the characteristics of German language. In addition we will
% compare  the performance with English language.


%  As word vectors are generated
% by \ac{NNLM} \cite{Bengio:2003:NPL:944919.944966}, it is expected therefore,
% t

% First  as reported in previous work, n-gram backoff language models, the state of the art
% technique, do not perform  as well for inflectional  languages as they work
% for English \cite{conf/icassp/MikolovKBGC09}. Recent alternatives
% n-gram based models are \ac{NNLM} \cite{Bengio:2003:NPL:944919.944966} described
% in detail in CHAPTER [TODO:REF]. Besides the language model,  \ac{NNLM}
% generate the word  vector representations .

% in this type of languages \cite{conf/icassp/MikolovKBGC09}\cite{DBLP:journals/corr/MikolovLS13}.
% Second, 

% As mentioned before,  vector representations have shown the ability of
% improve existing \ac{NLP} taks 

% -  There is no studying comparing performances in two different languages and
% how the differences between the language and the standard language affects. 



%  [Thurian and
% Other people papers listed on Mikolov]

% - This work have done in english (not studied in depth in high inflectional
% languages) however there are not study which theoritically explains what 

% - There are some evidence of good performance morphologucally rich (highly
% inflective) lanaguage 

% %
%- [On the other hand, n-gram backoff models do not work
% as well for inﬂectional languages as they work for
% English] \cite{conf/icassp/MikolovKBGC09} 

%  Several alternative model has been proposed based on \ac{NNLM}

% - As the word embeddings are based also en \ac{NNLM} it is expected that
% thereofore works better. 

% - We use in our work German is a morpholgically rich and moderate inflected
% language (CITATION). We want to test evaluate the performance of this
% particular word embeddings. 

% Similar to the English study we define a set of tasks 

% To evaluate a language model is necessary define a set of tasks 
% For that purpose, a equivalent subset of tasks targeted at the German language were
% defined in the same maner as by Mikolov et al.
% \cite{DBLP:journals/corr/abs-1301-3781} do for the English.  


% It is challenging to test and compare different language models base just by
% comparing their algorithm description, furthermore if for the language of
% choosing no standard data set exist. For that purpose a equivalent  to the
% one defined by  has been created. 

% This chapter will describe the experiments performed as well as the data sets
% used to test the model. Additionally, a comparison between

% To test this model for  German,  a equivalent set of semantic and
% syntantic tasks were created in a similar way to the ones defined by Mikolov
% et al \cite{MikolovSCCD13}.

% This chapter describes the experiments and and tasks used to measure  

% IMPORTANT ALSO FOR GINI SET - [Neural networks seem to be very useful for tasks where
% we struggle with inflectional languages and limited data
% amounts. While performance of our implementation is not
% high,  presents various optimization techniques that lead to
% significant improvements in required computational time for
% training and lattice re-scoring.] - \cite{conf/icassp/MikolovKBGC09}

\section{Task Description}

In their original paper \cite{DBLP:journals/corr/abs-1301-3781} the authors
introduced a comprehensive test that contained five types of semantic
questions. In the original data set  there were 8869 semantic and 10675
syntactic questions. To create this set the authors compiled a list of
similar word pairs manually and then they generated the list by
connecting the word pairs. Table~\ref{tab:task_original_en} shows examples 
examples from each category of tasks. 

To evaluate the quality of the word vectors the question  are answer in the
following manner: given the word pairs ($a$, $b$) and ($c$, $d$) a question
is considered to be answered correctly if the for resulting vector $X$ the closest vector measured in
cosine similarity is  $vector(d)$. $X$ is calculated by applying the following
operation $X = vector(b) - vector(a) + vector(c)$.  

 For example  for the words
($``small''$, $''smaller''$ ) and ($``{big}''$,$"bigger"$),
the $vector("bigger")$ should be the closest to the resulting
vector $X$. If the representation of the word in
vector space is good, then it is possible to find the answer using such
operation \cite{DBLP:journals/corr/abs-1301-3781}.

As discussed in the first part of this work, the word representations
capture different types of  similarities among words. Therefore, with the
operation described above,  what is  being
attempted is to answer the question ``\emph{What is the word that is similar to
c in the same sense as b is similar to a?}''. 

% To compare the quality of different versions of word vectors, previous papers typically use a table
% showing example words and their most similar words, and understand them intuitively. Although
% it is easy to show that word France is similar to Italy and perhaps some other countries, it is much
% more challenging when subjecting those vectors in a more complex similarity task, as follows. We
% follow previous observation that there can be many different types of similarities between words, for
% example, word big is similar to bigger in the same sense that small is similar to smaller. Example
% of another type of relationship can be word pairs big - biggest and small - smallest [20]. We further
% denote two pairs of words with the same relationship as a question, as we can ask: ”What is the
% word that is similar to small in the same sense as biggest is similar to big?”

In order to evaluate the performance of the word vectors in the same way but in other language a
similar set of questions  needs to be created. Two approaches might be used
to accomplish this. The words can be translated, either automatically or by hand, or the data set can be
generated from scratch in a similar manner as the original was.  In this work
the translation was performed by hand using an online dictionary \footnote{http://dict.cc} as guide and
verified by two native speakers.

Regardless of the method chosen, some tasks of this set are biased towards
the morphology of English and could not be easily mapped to another language,
and in particular to German. Therefore a simple translation would not be
enough to obtain a equivalent task set for another language. For that reason
each of the original 14 tasks was analyzed to either adapt it to German or
discard it completely. 


\renewcommand{\arraystretch}{1.3}

\begin{table}[h]\
  \caption{Examples of the original types of syntactic questions in the
    Semantic-Syntactic Word Relationship test as defined
    by \cite{DBLP:journals/corr/abs-1301-3781}.}
  \label{tab:task_original_en}

  \begin{tabular}{ |l| |c|*{4}{c| |c| c | c }  }

  \hline           
  Type of Relationship &  \multicolumn{2}{c||}{Word Pair 1} &
  \multicolumn{2}{c|}{Word Pair 2} \\  \hline           
  Common capital city & Athens & Greece & Oslo  & Norway \\ 
  All capital cities  & Astana & Kazakhstan &  Harare & Zimbabwe  \\
  Currency & Angola & kwanza & Iran & rial \\  
  City-in-state  & Chicago & Illinois & Stockton & California \\  
  Man-Woman & brother & sister  & grandson & granddaughter \\  \hline  
  Adjective to adverb & apparent & apparently & rapid & rapidly  \\  
  Opposite & possibly & impossibly & ethical & unethical \\  
  Comparative & great & greater & tough & tougher \\  
  Superlative & easy & easiest & lucky & luckiest \\  
  Present participle & think & thinking & read & reading \\  
  Nationality adjective & Switzerland & Swiss & Cambodia  & Cambodian  \\  
  Past tense & walking & walked & swimming & swam \\ 
  Plural nouns  & mouse & mice & dollar & dollars \\  
  Past verbs & work & works & speak  & speaks  \\  \hline  
  
    
  
  
\end{tabular}
\end{table}

\section{Adapting the tasks to the German Language}
\label{sec:adapt_task_german_lang}

Many of  tasks from the original  Semantic Syntactic set of task are language
dependent. In order to adapt them  to the German language  each of different
types of question was evaluated. Some of them can be easily adapted to German
just by translating the respective word. However, some of the them either do
not make sense for German in from the morphology point of view or do not
apply for German. From the original 14 types of questions, the German version only
10 and equivalent set was also constructed for English for performing the
comparison. Below each of the tasks are described in detail along with what
was done to adapt it to German when possible.

\subsection{Common Capital City and All Capital}
\label{sec:sub_sec_common_capital_country}

This group of question try to solve analogies between country and capitals.
The vector model is asked the semantic relationship \emph{Capital Country}.
For example:  ``\emph{What is the word that is similar to
Germany in the same sense as Paris is similar to France?}''. The model should
answer then the word produce a vector close to ``\emph{Berlin}''. The
difference between the two subsets is that common capital contains
 the 22 most ``\textit{common}'' country - capital word pairs. However the
 criteria for the selection is not defined by the authors of the set. The
 other subset contains country - capital pairs of less known countries such
 as \textit{Ghana  -  Accra} and  \textit{Harare -  Zimbabwe}.

For this task the adaptation was done by simply translating the names from
the English ones to their German counterparts. 

\subsection{Currency}
\label{sec:sub_sec_currency}

% To measure quality of the word vectors, we  a comprehensive test set that contains ﬁve types
% of semantic questions, and nine types of syntactic questions. Two examples from each category are
% shown in Table 1.


%% • private data sets that do not allow experiments to be repeated are used
%% • ad hoc preprocessing is used that favours the proposed technique, or completely
%% artificial data sets are used
%% • comparison to proper baseline is completely missing
%% • baseline technique is not tuned for the best performance
%% • in comparison, it is falsely claimed that technique X is the state of the art (where X is usually n-gram model)
%%  possible comparison to other advanced techniques is done poorly, by citing results achieved on different data sets, or simply by falsely claiming that the other techniques are too complex 

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../main.tex"
%%% End:
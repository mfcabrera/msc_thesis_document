\chapter{Empirical Study of Word Vectors for the German Language}
\label{chapter:wor2vec_german}

%% In this chapter 

%% - What we do in this chapter 

%% - Rationale why we do that how we do it and why

%% - Short descrioption of Experiments

%% Tasks for English and German and comparisson of Gramatical and semantical differences

%% Datasizes and so on

%% - Embedding using t-SNE of German wikipedia (top X)

%% - Embedding using t-SNE of GiniData using top X

%% - Table of the experiments

%% - Analysis of the results 

The previous chapters introduced the concepts and related work  in the fields
of  \ac{NLP}, \ac{NNLM} and vector representation of words. This chapter will
focus on the application of a particular model, namely \textit{Word2Vec}, to
the German language.  

% There art two main reasons that justify the study of the behavior of word
% vector representation in a language as morphologically rich and  inflective
% as German.

Although that  evidence that word vector representation might work
well in other languages other than English has been presented
\cite{DBLP:journals/corr/MikolovLS13}, to the date  there are not in depth studies or empirical evaluations of the
performance of word representation for such languages,  in similar or equivalent tasks as the
ones performed by \cite{DBLP:journals/corr/abs-1301-3781} for the English
language. 

Previous work has shown that  n-gram backoff language models, the state of the art
technique, does not perform  as well for inflectional  languages as they work
for English \cite{conf/icassp/MikolovKBGC09}.  Previous work also shows that
\ac{NNLM} captures  better the complexity of such languages
\cite{conf/icassp/MikolovKBGC09}\cite{DBLP:journals/corr/MikolovLS13},
therefore  is possible that word vectors generated by such model  will 
improve  and simply many \ac{NLP} applications in
these languages as they have been shown to do in
English \cite{collobert:2008} \cite{Turian:2010:WRS:1858681.1858721}.  This makes therefore an initial evaluation of the quality of the word vector
attractive.

The rest of the chapter is divided as follows: The first part described the
selected tasks, the rationale behind the adaption made to fit the German
language and the data set  and the different preprocessing alternatives we
tried. The second part described the training approach as well describe the most
important result of our approach.

%  In this chapter we will
% describe a set of tasks adapted from \cite{DBLP:journals/corr/abs-1301-3781}
% to better match the characteristics of German language. In addition we will
% compare  the performance with English language.


%  As word vectors are generated
% by \ac{NNLM} \cite{Bengio:2003:NPL:944919.944966}, it is expected therefore,
% t

% First  as reported in previous work, n-gram backoff language models, the state of the art
% technique, do not perform  as well for inflectional  languages as they work
% for English \cite{conf/icassp/MikolovKBGC09}. Recent alternatives
% n-gram based models are \ac{NNLM} \cite{Bengio:2003:NPL:944919.944966} described
% in detail in CHAPTER [TODO:REF]. Besides the language model,  \ac{NNLM}
% generate the word  vector representations .

% in this type of languages \cite{conf/icassp/MikolovKBGC09}\cite{DBLP:journals/corr/MikolovLS13}.
% Second, 

% As mentioned before,  vector representations have shown the ability of
% improve existing \ac{NLP} taks 

% -  There is no studying comparing performances in two different languages and
% how the differences between the language and the standard language affects. 



%  [Thurian and
% Other people papers listed on Mikolov]

% - This work have done in english (not studied in depth in high inflectional
% languages) however there are not study which theoritically explains what 

% - There are some evidence of good performance morphologucally rich (highly
% inflective) lanaguage 

% %
%- [On the other hand, n-gram backoff models do not work
% as well for inﬂectional languages as they work for
% English] \cite{conf/icassp/MikolovKBGC09} 

%  Several alternative model has been proposed based on \ac{NNLM}

% - As the word embeddings are based also en \ac{NNLM} it is expected that
% thereofore works better. 

% - We use in our work German is a morpholgically rich and moderate inflected
% language (CITATION). We want to test evaluate the performance of this
% particular word embeddings. 

% Similar to the English study we define a set of tasks 

% To evaluate a language model is necessary define a set of tasks 
% For that purpose, a equivalent subset of tasks targeted at the German language were
% defined in the same maner as by Mikolov et al.
% \cite{DBLP:journals/corr/abs-1301-3781} do for the English.  


% It is challenging to test and compare different language models base just by
% comparing their algorithm description, furthermore if for the language of
% choosing no standard data set exist. For that purpose a equivalent  to the
% one defined by  has been created. 

% This chapter will describe the experiments performed as well as the data sets
% used to test the model. Additionally, a comparison between

% To test this model for  German,  a equivalent set of semantic and
% syntantic tasks were created in a similar way to the ones defined by Mikolov
% et al \cite{MikolovSCCD13}.

% This chapter describes the experiments and and tasks used to measure  

% IMPORTANT ALSO FOR GINI SET - [Neural networks seem to be very useful for tasks where
% we struggle with inflectional languages and limited data
% amounts. While performance of our implementation is not
% high,  presents various optimization techniques that lead to
% significant improvements in required computational time for
% training and lattice re-scoring.] - \cite{conf/icassp/MikolovKBGC09}

\section{Task Description}


In their original paper \cite{DBLP:journals/corr/abs-1301-3781} the authors
introduced a comprehensive test that contained five types of semantic
questions. In the original data set  there were 8869 semantic and 10675
syntactic questions. To create this set the authors compiled a list of
similar word pairs manually and then they generated the list by
connecting the word pairs. Table~\ref{tab:task_original_en} shows examples 
examples from each category of tasks. 

To evaluate the quality of the word vectors the question  are answer in the
following manner: given the word pairs ($a$, $b$) and ($c$, $d$) a question
is considered to be answered correctly if the for resulting vector $X$ the closest vector measured in
cosine similarity is  $vector(d)$. $X$ is calculated by applying the following
operation $X = vector(b) - vector(a) + vector(c)$.  

 For example  for the words
($``small''$, $''smaller''$ ) and ($''big''$ ,$"bigger"$),
the $vector("bigger")$ should be the closest to the resulting
vector $X$. If the representation of the word in
vector space is good, then it is possible to find the answer using such
operation \cite{DBLP:journals/corr/abs-1301-3781}.

As discussed in the first part of this work, the word representations
capture different types of  similarities among words. Therefore, with the
operation described above,  what is  being
attempted is to answer the question ``\emph{What is the word that is similar to
c in the same sense as b is similar to a?}''. 

% To compare the quality of different versions of word vectors, previous papers typically use a table
% showing example words and their most similar words, and understand them intuitively. Although
% it is easy to show that word France is similar to Italy and perhaps some other countries, it is much
% more challenging when subjecting those vectors in a more complex similarity task, as follows. We
% follow previous observation that there can be many different types of similarities between words, for
% example, word big is similar to bigger in the same sense that small is similar to smaller. Example
% of another type of relationship can be word pairs big - biggest and small - smallest [20]. We further
% denote two pairs of words with the same relationship as a question, as we can ask: ”What is the
% word that is similar to small in the same sense as biggest is similar to big?”

In order to evaluate the performance of the word vectors in the same way but in other language a
similar set of questions  needs to be created. Two approaches might be used
to accomplish this. The words can be translated, either automatically or by hand, or the data set can be
generated from scratch in a similar manner as the original was.  In this work
the translation was performed by hand using a German - English online dictionary \footnote{http://dict.cc} as guide and
verified by two native speakers.

Regardless of the method chosen, some tasks of this set are biased towards
the morphology of English and could not be easily mapped to another language,
and in particular to German. Therefore a simple translation would not be
enough to obtain a equivalent task set for another language. For that reason
each of the original 14 tasks was analyzed to either adapt it to German or
discard it completely. 


\renewcommand{\arraystretch}{1.3}

\begin{table}[h]
  \centering
  \caption{Examples of the original types of syntactic questions in the
    Semantic-Syntactic Word Relationship test as defined 
    by \cite{DBLP:journals/corr/abs-1301-3781}. }
  \label{tab:task_original_en}

  \begin{tabular}{ |l| |c|*{4}{c| |c| c | c }  }

  \hline           
  Type of Relationship &  \multicolumn{2}{c||}{Word Pair 1} &
  \multicolumn{2}{c|}{Word Pair 2} \\  \hline           
  Common capital city & Athens & Greece & Oslo  & Norway \\ 
  All capital cities  & Astana & Kazakhstan &  Harare & Zimbabwe  \\
  Currency & Angola & kwanza & Iran & rial \\  
  City-in-state  & Chicago & Illinois & Stockton & California \\  
  Man-Woman & brother & sister  & grandson & granddaughter \\  \hline  
  Adjective to adverb & apparent & apparently & rapid & rapidly  \\  
  Opposite & possibly & impossibly & ethical & unethical \\  
  Comparative & great & greater & tough & tougher \\  
  Superlative & easy & easiest & lucky & luckiest \\  
  Present participle & think & thinking & read & reading \\  
  Nationality adjective & Switzerland & Swiss & Cambodia  & Cambodian  \\  
  Past tense & walking & walked & swimming & swam \\ 
  Plural nouns  & mouse & mice & dollar & dollars \\  
  Past verbs & work & works & speak  & speaks  \\  \hline  
  
    
  
  
\end{tabular}
\end{table}

\section{Adapting the tasks to the German Language}
\label{sec:adapt_task_german_lang}

Many of  tasks from the original  Semantic Syntactic set of task are language
dependent. In order to adapt them  to the German language  each of different
types of question was evaluated. Some of them can be easily adapted to German
just by translating the respective word. However, some of the them either do
not make sense for German in from the morphology point of view or do not
apply for German. From the original 14 types of questions, the German version only
10 and equivalent set was also constructed for English for performing the
comparison. Below each of the tasks are described in detail along with what
was done to adapt it to German when possible.

\subsection{Common Capital and City and All Capital}
\label{sec:sub_sec_common_capital_country}

This group of question try to solve analogies between country and capitals.
The vector model is asked the semantic relationship \emph{Capital Country}.
For example:  ``\emph{What is the word that is similar to
Germany in the same sense as Paris is similar to France?}''. The model should
answer then the word produce a vector close to ``\emph{Berlin}''. The
difference between the two subsets is that common capital contains
 the 22 most ``\textit{common}'' country - capital word pairs. However the
 criteria for the selection is not defined by the authors of the set. The
 other subset contains country - capital pairs of less known countries such
 as \textit{Ghana  -  Accra} and  \textit{Harare -  Zimbabwe}.

For this task the adaptation was done by simply translating the names from
the English ones to their German counterparts. 

\subsection{Currency}
\label{sec:sub_sec_currency}
For the currency section all of the pairs were translated using the wikipedia
as source for the information. 

\subsection{City in State}
\label{sec:sub_sec_city_in_state}
This group of questions try evaluate the semantic relationship between a city
and a state for cities in the United States of America.  However, the
objective of the experiment is to have a standard,  almost
language independent comparable test set for the word vectors generated by \textit{Word2Vec}, tasks that
are centered in information mostly found in a specific language are not relevant and
will not provide good information about the quality of the vectors. For this
reason the complete tasks group was removed from the dataset. An alternative could have been to replace it with a similar
tasks (e.g. using cities in Germany / Austria). In this case however,  a
comparisson with another language would have been impossible as the
information sources might not contain that information or even the word
themselves.

\subsection{Man-Woman}
\label{sec:sub_sec_man_woman}
This tasks try to evaluate the semantic relationship between words associated
with men and women. For this tasks the equivalent German version of the word
were used for validation a  online German - English dictionary was used.

\subsection{Adjective to Adverb}
\label{sec:sub_sec_adjetive_adverb}
This task models the relationship between an adjective and its correspondent
adverb. However in German this task cannot be applied,  because unlike English,
German does not make a distinction in form between a predicate adjective and
an adverb \cite{durrell2011hammer}. For example in English:

\begin{itemize}
\item The man is quiet. (quiet = adjective)
\item The man sings quietly. (quietly = adverb)
\end{itemize}

Whereas a German speaker would say:

\begin{itemize}
\item Der Mann ist leise. (leise = adjective)
\item Der Mann singt leise. (leise = adverb)
\end{itemize}

For this reason the trained word vector will not be able to differentiate the
roles. Thus, this tasks is removed from the set. 

\subsection{Opposite}
\label{sec:sub_sec_opposite}
This task try to model the \emph{opposite} semantic relationship between words.
Although it may seem relatively easy to adapt this task to German there
things that need to bee taken into account. In German,  not necessarily the
most common ``antonym'' is used nor they have a regulara morphological
structure as in English. For example  in English is usual to form the
opposite by adding the prefix \textit{in} ,  \textit{un} , \textit{im} or
\textit{dis}. In fact, all of the word pairs that appear in the English
version of this set are formed in such way. This is not always true in
German. For example, the opposite for the word  \textit{\"{u}berzeugend} (roughly
translating to \emph{convincing} in English) has not opposite in that form.
The only way to form the opposite is by adding the negation, e.g.
\textit{nicht  \"{u}berzeugend }. Another example would be the pair
\textit{geschmackvoll} - \textit{ geschmacklos}, which are as well opposites,
but where the suffix is the difference.
However, most of the word in these tasks could be also build in a similar
fashion as in the English set, therefore we adapt this task by looking for
the best translations that can be formed with that common structure.

\subsection{Comparative and Superlative}
\label{sec:sub_sec_comparative_sup}

The comparative task  try to model to evaluate the syntactic relationship between a word
and its comparative form, for example the word pairs  \textit{young} and
\textit{younger}. Respectively the superlative task aims to evaluate the
superlative relationship  (e.g. \textit{safe}  and \textit{safest}). 

Many of the words belonging to this data set can be easily translated to
German directly. However, there are some caveats. First, English have many
more words for representing the same semantic concept than German.  English is language from
the family of Germanic languages, therefore shares the common structure
and grammar. However, many words have been borrowed from Latin, expanding the
vocabulary  and in many cases replacing the words from Germanic origin. 
For example, the word \textit{tall}  comes from Old Germanic and  the
word \textit{large} comes from Latin. Both words could be translated to
German as \textit{hoch}. However, the English word  \textit{high} of German
origin can also be translated as \textit{hoch}.
% Another example of this is the German the word  \textit{gross}, that represents
% the quality  of being  \textit{big}, but also \textit{large} or \textit{tall}.
That is a unique concept in German can represent many things in English. 
Give this, only one word is maintained in German and one English representing
the same concept making the number of pairs equivalent.


% To measure quality of the word vectors, we  a comprehensive test set that contains ﬁve types
% of semantic questions, and nine types of syntactic questions. Two examples from each category are
%
\subsection{Present Participle}
\label{sec:sub_sec_present_participle}

This tasks try to model the relationship between verbs and its participle
(e.g. \textit{come} -\textit{coming}). In English is used the participle in
much more ways than German: 

\begin{itemize}
\item  To form the progressive (continuous) aspect: Jim was sleeping. as an
  adjective phrase modifying a noun phrase: The man sitting over there is my
  uncle.
\item adverbially, the subject being understood to be the same as that
  of the main clause: Looking at the plans, I gradually came to see where the
  problem lay.
\item  similarly, but with a different subject, placed before the
  participle (the nominative absolute construction): He and I having
  reconciled our differences, the project then proceeded smoothly.
\item more
  generally as a clause or sentence modifier: Broadly speaking, the project
  was successful.

\end{itemize}

In German, unlike English  the present participle is used almost exclusively as an
adjective or adverb and as such are inflected. This is imply that in a model
such as  \textit{Word2Vec} the same word after the declension will be
represented with different words. For the purpose of this tasks, this mean
the is expected to have low number of correct answers.

\subsection{Nationality Adjective and  Plural nouns}
\label{sec:sub_sec_nat_plu}

The nationality adjective tasks model the relationship between a country and
the the adjective representing the nationality. For example, for the word
\textit{Colombia}, the respective adjective is \textit{Colombian}. In German,
the nationality is also an adjective, and as such, it  is inflected based on the
case and genus of the substantive.  For example  \textit{Kolumbianisch} may appear 
inflected as \textit{Kolumbianischer},  \textit{Kolumbianischem},
\textit{Kolumbianische}, etc.  Similar to the participle, many words will
represent the same concept, thus low correct answer rate are expected in this
task.  
The plural nouns task model the relationship of a word and its plural the
proportion of words that are the same plural is higher than English. This is
a factor that will affect the test results because the same word is never taken as a
valid answer. 


\subsection{Past Tense and Plural Verbs}
\label{sec:sub_sec_plural_verbs}
The two task find syntantic relationship between verbs and they past tense
and the conjugation respectively. This relationships does not hold in German,
and therefore are removed from the test.

\section{Summary of Changes} 
Table \ref{tab:summary_of_changes_task} shows a summary of the changes
performed in other to generate a set for the German language.

 \begin{table}[h]
   \centering
   \caption{Summary of changes } 
   \label{tab:summary_of_changes_task}

   \begin{tabular}{ |l|l|  }

   \hline           
   Task  & Action   \\  \hline           
   Common capital city & Translated \\
   All capital cities  & Translated \\
   Currency & Translated  \\ 
   City-in-state   &  Removed - Language dependent \\
   Currency & Translated  \\
   Man-Woman  & Translated  \\
   Adjective to adverb  &  Removed - do not apply in German \\
   Comparative & Translated - Some words removed due to polisemy \\ 
   Superlative & Translated - particle \textit{am} not used \\
   Opposite  & Translated \\
   Present participle & Translated \\
   Plural Nouns  & Translated \\
   Past verbs &  Removed \\
   Nationality adjective & Translated \\
   
\hline

  
\end{tabular}
\end{table}
    
In parallel to this changes, the english version was also modified to match
this new set of tasks so a comparisson could be performed in a somewhat
equivalent manner.

\section{Description of Experiments}
\label{sec:german_eng_experiments}

\subsection{Dataset}
\label{experiments:sub:dataset}

In the original paper \cite{DBLP:journals/corr/abs-1301-3781} the author
trained the word vectors using Google News internal data set containing about
6 billions of tokens. This data set is not available. The authors have made
available the pre-trained 300-dimensional  word vector  using Google internal
data set containing 3 million words and phrases.

The closest open data set available with such variety is the Wikipedia, and
we use both, the English and the German Wikipedia for our experiments.  It is
important to notice, that as it happens with most of the information
available in World Wide Web, there are much more information in English that
in German.  As discussed before, the amount of data available affects the quality  of the trained
word vectors. 

For the English and German word vector model  the Wikipedia is used. For
German the complete article dump of the 24$^{th}$ of October of 2013  of the
Wikipedia is used.  For English, the dump of all the articles from the
4$^{th}$ of November of 2013 is used. The dates are reported for informative
purpose. The data set can be obtained from the Wikipedia
servers.\footnote{\url{http://dumps.wikimedia.org/}}.

As this dump are in XML and contains characters and words that are not of
interest when building the word vector  representation the following clean up
procedure was applied to them:

\begin{itemize}
\item Remove all XML/HTML tags from the file preserving text and caption for
 URLs and images.
\item Remove special characters such as \&,  [, \}, etc.
\item make all characters lowercase.
\item spell out numbers.
\item remove any other character that is not alphanumeric.
\end{itemize}

In addition to this, for the German language the additional modification were
performed
\begin{itemize}
\item Transform \textit{umlauts} to the ASCII spelling.  e.g. \"{o} is
  transformed to \textit{oe},  \"{u} is transformed to \textit{ue}  and so
  on. 
\item The \textit{Eszett} or \textit{scharfes S} (Symbol '\ss') \  is transformed to ss.
\end{itemize}

Table shows \ref{tab:summ_dataset_germanword2vec} the data sets used
to train the file the word vectors with their size and number of tokens after
the preprocessing. 
  

 \begin{table}[h]
   \centering
   \caption{Summary of Data sets} 
   \label{tab:summ_dataset_germanword2vec}

   \begin{tabular}{ |l|c|c| }
   \hline           
    File Name &  Size (GB) & Number of tokens \\  \hline           
    dewiki-20131024-pages-articles.txt & 6579 &   1051584822 \\
    enwiki-20131104-pages-articles.txt & 17412 &  3177449048  \\
    \hline

\end{tabular}
\end{table}



We can see that the English dataset contains almost 3 times as tokens as the
German one. This mean that for the English word vector model  there much more
setences to train form, and therefore can be expected a better performance
from this model in the taks described before. 
  
\subsection{Training of the Models}
\label{experiments:sub:Training}

In order to train the model the public \emph{C} implementation available of
\textit{word2vec} \footnote{\url{https://code.google.com/p/word2vec/}} was
used . The implementation was slightly modified to account for the different
number to task categories and to provide more debugging information. 

In a similar fashion as in the original paper in order to quickly train the
model we evaluted the learned model on the 30.000 most common words. That
gave us 





%%%%% 
%%% %TODO: Declination in German additonal task.
%%%% Inflection of der die das
%%%% Inflrection of adjectives



%% • private data sets that do not allow experiments to be repeated are used
%% • ad hoc preprocessing is used that favours the proposed technique, or completely
%% artificial data sets are used
%% • comparison to proper baseline is completely missing
%% • baseline technique is not tuned for the best performance
%% • in comparison, it is falsely claimed that technique X is the state of the art (where X is usually n-gram model)
%%  possible comparison to other advanced techniques is done poorly, by citing results achieved on different data sets, or simply by falsely claiming that the other techniques are too complex 

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../main.tex"
%%% End:
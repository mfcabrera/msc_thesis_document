% Abstract for the TUM report document
% Included by MAIN.TEX


\clearemptydoublepage
\phantomsection
\addcontentsline{toc}{chapter}{Abstract}	





\vspace*{2cm}
\begin{center}
{\Large \bf Abstract}
\end{center}
\vspace{1cm}

The success of machine learning algorithms  depends on the 
representation of the data used.  Specific domain knowledge can be used to
design good representations. However, these representations are limited to a
specific problem  or task, and to the amount of available labeled data.
Another approach is to automatically learn  generic priors that can be used in different
tasks and context. In the field of natural language processing, recent work
has been done in obtaining such priors by learning useful vector representation of
words from unlabeled data. The representations can then be used to improve
existing natural language processing systems.  These word vectors are  obtained using special neural network architectures   
 trained on billions of tokens. However, most of these models  are learned
and evaluated on English language corpora.       In this work,
\textit{Word2vec}, a recent neural network based  toolkit for learning word
representations is used on German language data. The goal is to evaluate the
learned representations of words in different language processing and information
retrieval tasks. In particular, a semantic-syntactic  evaluation set  is
constructed for the German language. In addition to that, the
learned word vector representations are used as features for a classifier of German
language business documents. The learned features outperformed existing handcrafted features and
performed  similar to other state-of-the-art approaches.




%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../main.tex"
%%% End:

